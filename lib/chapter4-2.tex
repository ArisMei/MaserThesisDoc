\subsection{Metodología Estudio 2}\label{sec:metodologia-estudio-2}

En este segundo estudio como se ha mencionado en la Sección~\ref{sec:estudio2}, se tomará en cuenta el conjunto de pacientes: \texttt{Valid\_patients\_P2}. El propósito de este estudio es investigar la viabilidad de predecir si un paciente requerirá Oxigenoterapia de Alto Flujo (OAF) en las siguientes $8$ horas después del ingreso del paciente.

Para ello habrá que utilizar $2$ herramientas estadísticas que permitan realizar la predicción de la variable de interés:

\begin{itemize}
    \item \textbf{\textit{Hierarchical Clustering} (HCLUST):} En primer lugar, de manera exploratoria, se agruparán los pacientes en función de las distancias euclídeas entre ellos utilizando el método de clustering \texttt{hclust}. Posteriormente, se evaluará si los grupos generados han aislado en un \textit{cluster} a los pacientes con necesidad de Oxigenoterapia de Alto Flujo (OAF) de aquellos que no la requieren.
    \item \textbf{\textit{Random Forest} Classification (RF):} En segundo lugar, se aplicará el método estadístico \textit{Random Forest} para clasificar a los pacientes según el cluster al que pertenecen, utilizando las variables descriptivas de la Tabla~\ref{tabla:cuali_cuanti}. En este proceso, no se emplearán las variables de \textit{Notas} ni el \textit{Identificador Paciente}. Si se logra aislar a los pacientes que han necesitado OAF, se determinará la importancia directa de la variable DETERIORO, la cual indica si el paciente ha experimentado OAF o no.
\end{itemize}

\subsubsection{Hierarchical Clustering (HCLUST)}\label{sec:hclust}

\textit{Hierarchical Clustering} (HCLUST) es una técnica de análisis de datos utilizada para agrupar objetos en función de sus similitudes o distancias. Esta técnica se basa en la construcción de un dendrograma, una estructura jerárquica de ramas que representa la agrupación gradual de los datos (\cite{jain1988algorithms}).

El proceso comienza considerando cada objeto como un grupo individual y luego fusionando iterativamente los grupos más cercanos en función de una medida de similitud o distancia. La distancia puede ser calculada de varias formas, siendo las más comunes y sencilla la distancia euclidiana: 

La fórmula general para calcular la distancia euclídea entre dos objetos \(p\) y \(q\) se expresa de la siguiente manera:

\[
d_{euc}(p,q) = \sqrt{(p_{1} - q_{1})^2 + (p_{2} - q_{2})^2}
\]

Esta ecuación puede ser generalizada a un espacio euclidiano de \(n\)-dimensiones donde cada punto está definido por un vector de \(n\) coordenadas: \(p = (p_{1},p_{2},p_{3},...,p_{n})\)

\[
d_{euc}(p,q) = \sqrt{(p_{1} - q_{1})^2 + (p_{2} - q_{2})^2 + \ldots + (p_{n} - q_{n})^2} = \sqrt{\sum_{i=1}^{n}(p_{i} - q_{i})^2}
\]

Hay diferentes distancias que se pueden usar en función del propósito de lo que se quiera hacer con los datos. Cada uno de ellos depende del tipo de datos que se esté utilizando. Se mostrará a continuación una pequeña selección de lo que está disponible en \texttt{R}.
\begin{table}[H]
    \centering
    \caption{Opciones de método de distancia}
    \begin{tabular}{|p{3cm}|p{2.5cm}|p{4.5cm}|p{6cm}|}
    \hline
    \textbf{Medida} & \textbf{Método} & \textbf{Mejor utilizado para...} & \textbf{Cálculo} \\
    \hline
    Distancia euclidiana & euclidean & Datos continuos & \(edist(x, y) = \sqrt{((x_1 - y_1)^2 + (x_2 - y_2)^2 + \ldots)}\) \\
    \hline
    Distancia de Hamming & hamming & Datos categóricos & \(hdist(x, y) = \sum((x_1 \neq y_1) + (x_2 \neq y_2) + \ldots)\) \\
    \hline
    Distancia de Manhattan & manhattan & Diferencias absolutas entre componentes correspondientes & \(mdist(x, y) = \sum(|x_1 - y_1| + |x_2 - y_2| + \ldots)\) \\
    \hline
    Distancia de Canberra & canberra & Valores no negativos (ej. conteos) & \(cdist(x, y) = \sum\left(\frac{|x - y|}{|x + y|}\right)\) \\
    \hline
    Distancia binaria asimétrica de Jaccard & binary & Datos binarios & \(j.dist(x, y) = b + c + d\) (Usando tabla siguiente) \\
    \hline
    \end{tabular}
\end{table}

Una vez que se haya creado una matriz de distancias, es necesario decirle al modelo \textit{HCLUST} que debe entender él como similitud. Esto lleva al próximo punto de decisión: seleccionar qué se entender por \textit{similitud} entre grupos de observaciones. Esto se define como función de enlace (linkage) que es la estrategia utilizada para calcular la distancia entre grupos o clústeres en el proceso de construcción de un dendrograma jerárquico.

Existen muchas opciones para calcular similitudes entre observaciones. Los métodos \textit{ward.D} y \textit{ward.D2} son generalmente populares y efectivos. Sin embargo, aquí se mostrarán algunas otras opciones diferentes. A continuación se muestra una lista de los métodos de similitud más comunes:

\begin{table}[H]
    \centering
    \begin{tabular}{|p{3.5cm}|p{6cm}|p{6cm}|}
    \hline
    \textbf{Método} & \textbf{Proceso} & \textbf{Resultado} \\
    \hline
    Single & Mide la distancia entre los dos puntos más cercanos en cada cluster & Generalmente es mejor para identificar valores atípicos que no se agrupan bien \\
    \hline
    Complete & Mide la distancia entre los dos puntos más distantes en cada cluster & Generalmente produce clusters más compactos \\
    \hline
    Centroid & Mide la distancia entre el centro de cada cluster & Generalmente funciona mejor para datos con menos similitudes \\
    \hline
    Mediana & Mide la distancia mediana entre el punto mediano de cada cluster & Similar al centroide, pero ponderado hacia donde se encuentran la mayoría de las observaciones \\
    \hline
    Promedio & Mide la distancia promedio entre cada observación en cada cluster, ponderado por el número de observaciones en cada cluster & Generalmente similar a enlace completo, mejor para incorporar valores atípicos \\
    \hline
    Mcquitty & Similar al promedio, pero no toma en cuenta el número de puntos en el cluster & Generalmente similar a enlace simple \\
    \hline
    Ward.D & Minimiza la varianza dentro de los clusters (suma de errores). Combina clusters según la menor distancia entre clusters & Generalmente produce clusters más compactos \\
    \hline
    Ward.D2 & Igual que Ward.D, pero las diferencias están al cuadrado (suma de errores al cuadrado) & Enfatiza las diferencias identificadas en Ward.D, haciendo que los clusters sean más diferenciables \\
    \hline
    \end{tabular}
    \caption{Métodos de enlace y sus resultados}
\end{table}

Una vez establecido que se entiende como similitud entre grupo de observaciones el resultado final se muestra en forma de dendrograma dónde los pacientes se agrupan gradualmente. Se puede seleccionar el número de grupos al cortar el dendrograma en diferentes alturas. Hay algoritmos que son capaces de seleccionar el número de grupos a considerar como \textit{silouhette} (Sección~\ref{sec:silhouette}) o \textit{gap statistic} que al final son puntuaciones que varían en función del número de \textit{clusters} que se haya decidido previamente.



En el contexto de este estudio, se utilizará el método de clustering \textit{HCLUST} para agrupar a los pacientes según sus distancias euclídeas considerando como similitud entre grupos de pacientes el método \textit{Ward.D2}. Esto permitirá investigar si los grupos generados pueden destacar diferencias entre pacientes que requieren Oxigenoterapia de Alto Flujo (OAF) y aquellos que no la necesitan. \textit{HCLUST} tiene la ventaja de ser visualmente intuitivo y permite explorar la estructura de los datos a diferentes niveles de detalle. Sin embargo, su complejidad computacional puede ser alta en conjuntos de datos grandes.

\paragraph{Silhouette}\label{sec:silhouette}

El método de \textit{Silhouette} es una técnica utilizada para determinar el número de clusters en un análisis cluster. Proporciona una medida de cómo de similar es un objeto a su propio cluster (cohesión) en comparación con otros clusters (separación). La puntuación de \textit{Silhouette} varía entre -1 y 1, donde una puntuación más alta indica que el objeto está bien clasificado en su propio cluster y mal clasificado en el cluster más cercano (\cite{rousseeuw1987silhouettes}).

La fórmula para calcular la puntuación de \textit{Silhouette} \(s(i)\) para un objeto \(i\) se define como:
\[ s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}},\]
donde:
\begin{itemize}
    \item \(a(i)\) es la distancia promedio entre el objeto \(i\) y todos los demás objetos en el mismo cluster (cohesión).
    \item \(b(i)\) es la distancia promedio entre el objeto \(i\) y todos los objetos en el cluster más cercano al que \(i\) no pertenece (separación).
\end{itemize}

La puntuación de \textit{Silhouette} para un conjunto de datos se puede calcular promediando las puntuaciones $s(i)$ de todos los objetos:
\[ \text{Puntuación de Silhouette Media} = \frac{1}{N} \sum_{i=1}^{N} s(i), \]
donde \(N\) es el número total de objetos en el conjunto de datos.

El número de clusters se elige cuando la puntuación de \textit{Silhouette} media es máxima. Se busca el número de clusters que produce una mayor cohesión dentro de los clusters y una mayor
separación entre los clusters vecinos, lo que refleja en una puntuación \textit{Silhouette} más alta.

\paragraph{Índice de Rand Ajustado (Adjusted Rand Index, ARI)}\label{sec:ari}

El \textit{Índice de Rand Ajustado} es una medida de la similitud entre dos particiones, como por ejemplo, dos grupos generados a partir de los mismos datos. Se utiliza comúnmente en la evaluación de la calidad de los resultados de algoritmos de clustering. 

El ARI ajusta el índice de Rand estándar para corregir la posibilidad de que la similitud entre dos particiones sea el resultado del azar. El Índice de Rand se define como: 

\[ RI = \frac{\text{Número de pares de elementos en la misma clase en ambas particiones}}{\text{Número total de pares de elementos en el conjunto de datos}} .\]

Cuanto mayor sea el valor del ARI, mejor será la concordancia entre las particiones, es decir ambos algoritmos han llegado a la conclusión que el agrupamiento de datos es similar. Cuanto más cerca el valor esté de 1 indica una concordancia perfecta entre los clusters generados (\cite{rand1971objective}).

\paragraph{FAS, FCC y Peridiograma}\label{sec:FAS-fcc-peridiograma}

Para realizar los clusters en el presente trabajo se han extraído de las \textit{Series Temporales} de \textit{Frecuencia Cardíaca} y \textit{Saturación de Oxígeno} los siguientes parámetros:

\subparagraph{Función Autocorrelación (FAS)}

La Función Autocorrelación (FAS) es una medida estadística que evalúa la similitud de una serie temporal con ella misma a lo largo de diferentes desplazamientos temporales. Se calcula mediante la siguiente fórmula:

\[
\text{FAS}(k) = \frac{\sum_{t=1}^{N-k}(X_t - \mu)(X_{t+k} - \mu)}{\sum_{t=1}^{N}(X_t - \mu)^2}
,\]

donde $N$ es el número de observaciones en la serie temporal, $X_t$ es el valor en el tiempo $t$, $\mu$ es la media de la serie y $k$ es el desplazamiento temporal.

La FAS proporciona información sobre la periodicidad y la correlación en la serie temporal. En el contexto de la extracción de características para la agrupación de datos, la FAC puede ayudar a identificar patrones de comportamiento repetitivos en las señales de Frecuencia Cardíaca y Saturación de Oxígeno (\cite{shumway2017chapter1}).

\subparagraph{Función Autocorrelación Cruzada (FCC)}

La Función Correlación Cruzada (FCC) es una medida que evalúa la similitud entre dos series temporales distintas a lo largo de diferentes desplazamientos temporales. Se calcula mediante la siguiente fórmula:

\[
\text{FCC}(k) = \frac{\sum_{t=1}^{N-k}(X_t - \mu_X)(Y_{t+k} - \mu_Y)}{\sum_{t=1}^{N}(X_t - \mu_X)(Y_t - \mu_Y)}
,\]

donde $N$ es el número de observaciones en ambas series temporales, $X_t$ y $Y_t$ son los valores en el tiempo $t$ de las dos series, $\mu_X$ y $\mu_Y$ son las medias respectivas de las series, y $k$ es el desplazamiento temporal (\cite{shumway2017chapter1}).

La FCC es útil para medir la relación entre las señales de Frecuencia Cardíaca y Saturación de Oxígeno. Puede ayudar a identificar cómo estas dos señales están relacionadas entre ellas en términos de sincronización y retrasos temporales.

\subparagraph{Peridiograma}

El Peridiograma se calcula mediante la Transformada de Fourier y se utiliza para identificar patrones de periodicidad en la serie. La fórmula básica de la Transformada de Fourier discreta es:

\[
X(f) = \sum_{t=1}^{N} x(t) \cdot e^{-2\pi i f t}
,\]

donde $X(f)$ representa la contribución de la frecuencia $f$, $x(t)$ es el valor de la serie temporal en el tiempo $t$, $N$ es el número de observaciones y $i$ es la unidad imaginaria. 

Mediante esta transformación, se descompone una señal en sus componentes de frecuencia, lo que facilita la identificación y el análisis de patrones cíclicos o periódicos dentro de la series temporales. Es una técnica muy utilizada y eficaz en el campo del procesamiento de señales y se utiliza ampliamente para comprender la estructura de las series temporales y sus características fundamentales. (\cite{schuster1898investigation}).

El Peridiograma puede revelar información importante sobre las frecuencias dominantes en las señales de \textit{Frecuencia Cardíaca} y \textit{Saturación de Oxígeno}. Esto es crucial para comprender la periodicidad en los datos y puede ser relevante para la agrupación de patrones de comportamiento en las series temporales.

En resumen, los parámetros de FAC, FCC y el análisis de Peridiograma se utilizan para extraer información valiosa de las series temporales de \textit{Frecuencia Cardíaca} y \textit{Saturación de Oxígeno}, para que se facilite la agrupación de datos mediante el método de clusterización anteriormente descrito y la identificación de patrones de interés en el contexto de este estudio.


\subsubsection{Random Forest (RF)}\label{sec:rf}

El procedimiento \textit{Random Forest}, también conocidos como bosques aleatorios, consiste en un conjunto de árboles de clasificación. Estos bosques se crean mediante un algoritmo que introduce variabilidad en los datos y las variables utilizadas, con el objetivo de reducir la correlación entre los árboles.

El algoritmo de clasificación basado en \textit{Random Forest} representa un avance con respecto a los árboles de decisión, en particular, el algoritmo CART (Classification and Regression Trees). La principal ventaja del \textit{Random Forest} radica en su mayor flexibilidad, lograda mediante la aplicación de técnicas de \textit{bagging} que generan múltiples versiones aleatorizadas de la base de datos a partir de subconjuntos de datos.

A diferencia de los árboles de decisión convencionales, donde se crea una única muestra aleatorizada, en \textit{Random Forest} se construye una muestra específica para cada árbol. Este proceso, conocido como Bootstrap, contribuye a la generación de estructuras de árbol altamente diversas, lo que aumenta la flexibilidad del algoritmo.

Para mejorar aún más los resultados de clasificación, es posible ajustar los hiperparámetros de \textit{Random Forest}. Estos hiperparámetros incluyen el número de variables consideradas en cada nodo, la cantidad de árboles en el conjunto (el bosque) y los criterios de detención. La optimización de estos parámetros puede tener un impacto significativo en la calidad de las clasificaciones obtenidas. En este contexto, se busca maximizar la eficacia del algoritmo para lograr una mejor precisión en la clasificación de datos.

El objetivo de los \textit{Random Forest} es predecir una variable respuesta \(y_i\) en función de \(p\) variables explicativas \(x_i = (x_{1i}, x_{2i}, \ldots, x_{pi})^T\), donde \(N\) representa el número total de observaciones en el conjunto de datos. Estos algoritmos, conocidos por su capacidad de manejar relaciones complejas entre variables, utilizan un enfoque de conjunto que combina múltiples árboles de decisión conocidos como CART. Cada árbol o CART se entrena en un subconjunto de datos generado mediante muestreo con reemplazo, y su predicción final se obtiene mediante la agregación de las predicciones individuales de todos los árboles. El conjunto de datos empleado para la estimación se denomina \(d = (X, y)\) (\cite{ho1995random}).

El propósito de los \textit{Random Forest} es predecir una variable de respuesta \(y_i\) en base a \(p\) variables explicativas \(x_i = (x_{1i}, x_{2i}, \ldots, x_{pi})^T\), siendo \(N\) el total de observaciones en el conjunto de datos. El conjunto empleado para la estimación se denota como \(d = (X, y)\) (\cite{ho1995random}).

\paragraph{CART: Árboles de Clasificación y Regresión}
Los árboles de clasificación (CART) son la base de los algoritmos \textit{Random Forest}. Estos dividen o segmentan el espacio de los predictores en un número limitado de regiones. Este enfoque pertenece al aprendizaje supervisado, donde se tiene una variable objetivo y se busca establecer una función clasificadora para predecir su valor. CART (Classification and Regression Trees) es una técnica que permite construir árboles de clasificación y de regresión. Se utiliza en clasificación cuando la variable objetivo es cualitativa y en regresión cuando es cuantitativa.

Dado un conjunto inicial de datos, un árbol de clasificación se construye creando una serie de particiones binarias. Cada partición se llama nodo y divide el espacio en dos partes según una variable. El objetivo es imponer restricciones a los datos a medida que descienden niveles en el árbol para clasificarlos en diferentes conjuntos (\cite{wu2008top}).

Para desarrollar un árbol, es necesario definir dos aspectos:
\begin{itemize}
	\item Cómo se selecciona el criterio de partición de un nodo.
	\item Criterios de parada que determinan cuándo finaliza el proceso de subdivisión.
\end{itemize}


\subparagraph{Criterio de Partición}
Supongamos que tenemos \(p\) variables explicativas o predictores: $(x_1, x_2,$ $\ldots, x_p$. Tomemos la primera variable \(x_1\) y busquemos el valor \(s\) que divide la muestra en dos regiones que cumplan:
\[
R1 = \{x_{1i} \mid x_{1i} < s\} \quad \text{y} \quad R2 = \{x_{1i} \mid x_{1i} \geq s\}
.\]

Es decir, o se toma una de las variables explicativas (o predictores) de tus datos y se busca un valor $s$ que divida las observaciones en dos grupos y que minimice:
\[
RSS_1(s) = \sum_{i \in R_1} (y_i - \bar{y}_{R_1})^2 + \sum_{i \in R_2} (y_i - \bar{y}_{R_2})^2,
\]
donde \(\bar{y}_{R_1}\) y \(\bar{y}_{R_2}\) son las medias de la variable \(y\) en las regiones \(R_1\) y \(R_2\), respectivamente. Esto se hace pues se pretende que en cada grupo la variable respuesta \(y\) sea lo más homogénea posible. Se busca el valor $s$ que minimicen la suma de los cuadrados de las diferencias entre los valores de $y$ y la media de $y$ en cada grupo.

Elegido \(s\) óptimo para \(x_1\), se repite el proceso con el resto de las variables. De las \(p\) variables, se elige para la partición la que resulte en la menor \(RSS_j\).

El proceso se repite, pero ahora se tratan de manera independiente los conjuntos \(R_1\) y \(R_2\). El proceso continúa hasta que se alcance el criterio preestablecido de parada y se definan los nodos terminales \(R_1^*, R_2^*, \ldots, R_j^*\). El objetivo es lograr una partición de estos nodos que minimice:
\[
RSS_T = \sum_{j=1}^{J} \sum_{i \in R_j^*} (y_i - \bar{y}_{R_j^*})^2
\]

\subparagraph{Criterio de Parada}
Existen varios criterios de parada para limitar el crecimiento del árbol:
\begin{itemize}
	\item Parámetro de Complejidad: Este parámetro se refiere al aumento en el valor de \(R^2\) al agregar una rama al árbol. Se establece un umbral para el cual agregar más ramas al árbol no aumenta significativamente el valor de \(R^2\).
	\item Minsplit: Se detiene el crecimiento del árbol cuando el número de observaciones en un nodo es menor que un umbral predefinido.
	\item Maxdepth: Se detiene el crecimiento cuando la profundidad de las ramas del árbol supera un umbral establecido.
\end{itemize}
La recomendación general es encontrar el árbol más pequeño que maximice el valor de \(R^2\).

\subparagraph{Ventajas de los CART}
Los CART presentan varias ventajas:
\begin{itemize}
	\item No se requiere preparación de los datos de entrada.
	\item Pueden manejar variables faltantes.
	\item Son efectivos para grandes bases de datos con diversas variables.
	\item Son fáciles de entender e interpretar.
\end{itemize}

\subsubsection{Algoritmo de Creación del \textit{Random Forest}}\label{sec:algoritmo-creacion-rf}

El proceso de creación del \textit{Random Forest} se lleva a cabo para \(b = 1,2, \ldots, B\) siguiendo los siguientes pasos:
\begin{enumerate}
  \item Se selecciona una muestra de tamaño \(N\) de manera aleatoria y con reemplazo del conjunto de datos total, lo que se conoce como bootstrap.
  \item Se construye un árbol denominado \(r_b(x)\). En cada nodo del árbol, se emplean \(m \ll p\) variables de las \(p\) variables disponibles en el conjunto de datos. La elección común para \(m\) es \(m = \sqrt{p}\).
  \item El conjunto de árboles formados, \(r_b(x)\) para \(b = 1,2, \ldots, B\), constituye el \textit{Random Forest}.
\end{enumerate}

\paragraph{División del Conjunto de Datos}

En términos generales, los algoritmos de Aprendizaje Automático dividen el conjunto de datos en dos partes: una para la etapa de entrenamiento y otra para la validación.

\paragraph{Estimación del Error en la Predicción de Variables de Respuesta Continuas}

La predicción utilizando \(x_i = (x_{1i}, x_{2i}, \ldots, x_{pi})^T\) se calcula como:
\[ \hat{y}_i = \hat{r}_{rf}(x_i) = \frac{1}{B} \sum_{b=1}^{B} \hat{r}_b(x_i),\]
donde \(B\) es el número de árboles en el \textit{Random Forest}.

El error en las predicciones, \(e_i\), es la diferencia entre el valor real \(y_i\) y la predicción estimada \(\hat{y}_i\). Es una medida usada para evaluar la precisión del modelo generado \textit{Random Forest}. 

\paragraph{Métricas de Evaluación en la Predicción Mediante Método \textit{Random Forest}}

El error cuadrático medio (MSE) se emplea para cuantificar el promedio de los errores al cuadrado para todas las observaciones en el conjunto de datos. Se calcula como:
\[ MSE = \frac{1}{n} \sum_{i=1}^{n} e_i^2.\]
El MSE proporciona una visión global de la calidad de las predicciones, esta métrica penaliza más los errores grandes que lo errores pequeños.

El coeficiente \(R^2\) es una métrica crucial para medir la proporción de la variabilidad total en los valores observados \(y_i\) que es capturada por las predicciones del modelo \(\hat{y}_i\). Su definición es:
\[ R^2 = 1 - \frac{\sum_{i=1}^{n} e_i^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}.\]
Donde \(\bar{y}\) es la media de los valores observados \(y_i\). Un valor de \(R^2\) más cercano a 1 indica que el modelo se ajusta bien a los datos, mientras que un valor bajo sugiere que el modelo no es capaz de explicar la variabilidad.

La desviación típica residual \({\hat{S_R}}\) proporciona una medida de la dispersión de los errores alrededor de las predicciones. Se calcula como:
\[ {\hat{S_R}} = \sqrt{\frac{e_i^2}{m-1}},\]
donde \(m\) es el número de observaciones en el conjunto de validación. Esta métrica ayuda a entender la variabilidad en los errores y a identificar posibles discrepancias significativas entre las predicciones del modelo y los valores reales.

En el contexto de \textit{Random Forest}, estas métricas mencionadas son fundamentales para evaluar el rendimiento del modelo y ajustar parámetros clave, como el número de árboles \(B\) y la profundidad máxima de los árboles individuales, con el objetivo de mejorar la precisión predictiva y controlar el sobreajuste que se pudiera generar.



\paragraph{OOB Error (Out of the Bag Error)}\label{sec:oob}

El OOB (Out-Of-Bag) error es una medida muy importante para evaluar la eficacia de modelos que emplean la técnica de boostrapping. Este enfoque,  utilizado en el contexto de Random Forests, implica la construcción de múltiples conjuntos de datos de entrenamiento mediante muestreo con reemplazo. Concretamente, en cada iteración del proceso de bootstrapping, aproximadamente \(1/3\) de los datos que no se seleccionan, lo que proporciona un conjunto de validación natural dentro del mismo proceso de entrenamiento. 

La métrica OOB error surge al aprovechar los datos no utilizados en cada iteración de bootstrap, con ellos se valida la muestra internamente durante el proceso de entrenamiento. Representa el error de predicción cometido por el modelo en los valores que fueron excluidos de la muestra de entrenamiento en una determinada iteración. La formulación del OOB error se puede expresar como:

\[
OOB\; error = \frac{1}{N} \sum_{i=1}^{N} L(y_i, \hat{y}_{i,\text{OOB}})
,\]

donde \(N\) es el tamaño total de la muestra, \(L\) es una función de pérdida que mide la discrepancia entre el valor real \(y_i\) y la predicción \(\hat{y}_{i,\text{OOB}}\) del modelo para la observación \(i\) utilizando solo los árboles que no utilizaron esa observación en su proceso de entrenamiento (\cite{james2013introduction}).


\paragraph{Importancia de las variables}\label{sec:importancia-variables}

Un modelo \textit{Random Forest} funciona como una caja negra y proporciona buenas predicciones. Sin embargo, no brinda información sobre cada variable explicativa.

La importancia de una variable se puede calcular en función de cómo afecta a la salida del modelo cuando se realizan cambios en las variables de entrada. A continuación, se describen dos formas de calcular la importancia.

\subparagraph{Índice de Pureza de los Nodos}

En el contexto de \textit{Random Forest}, el Índice de la Pureza de los Nodos se refiere al proceso de evaluación de la contribución de cada variable al proceso de particionamiento de los datos en los nodos de cada árbol. Al utilizar una variable en un nodo, se cuantifica la reducción de la variabilidad explicada por la partición resultante. Estos valores de reducción se acumulan para todas las variables y árboles, generando una medida conocida como la importancia de la variable. Las variables que exhiben una mayor capacidad para reducir la variabilidad en los nodos se consideran más importantes en el proceso de toma de decisiones del \textit{Random Forest}.

\subparagraph{Incremento del Mean Squared Error (\texttt{\%IncMSE})}

El índice \texttt{\%IncMSE} en el contexto de \textit{Random Forest} mide el incremento en el Error Cuadrático Medio (MSE) a través de la realización de una permutación aleatoria en una de las variables de entrada. La disparidad entre el MSE antes y después de la permutación se normaliza y se promedia para determinar la relevancia de la variable en el proceso de modelado. Este índice proporciona una evaluación cuantitativa de cómo la perturbación de una variable afecta la precisión de las predicciones, identificando así las variables que tienen un mayor impacto en los resultados del \textit{Random Forest}. Una variable que su permutación genere una gran variación será considerada como importante. Por el contrario, una variable que no afecte la precisión de las predicciones será considerada como no importante.

\paragraph{Hiperparámetros}\label{sec:importancia-variables}

\subparagraph{\textit{num\_trees}}

Este hiperparámetro hace referencia a la cantidad de árboles que se construirán en el \textit{Random Forest}. Un mayor número de árboles de manera general tiende a mejorar la estabilidad del modelo y reduce el riesgo de sobreajuste.

\subparagraph{\textit{mtry}}

El hiperparámetro \textit{mtry} determina cuántas variables predictoras se seleccionarán aleatoriamente en cada división de cada árbol. En la Sección~\ref{sec:algoritmo-creacion-rf} en el puntos $2$. Se hacía referencia a esta variable como  \(m \ll p\), dónde \(m\) es el número de variables predictoras y \(p\) es el número total de variables predictoras. La elección del valor del hiperparámetro  \textit{mtry} es importante, ya que determina la cantidad de variabilidad introducida en el proceso de construcción de cada árbol. 

\subparagraph{\textit{max\_depth}}
El hiperparámetro \textit{max\_depth} establece la profundidad máxima que puede alcanzar cada árbol generado dentro de el proceso \textit{Random Forest}. Un árbol con una profundidad es capaz de capturar relaciones más complejas entre las variables predictoras y la variable de respuesta. Sin embargo, esto también puede sobreajustar los datos de entrenamiento, lo que resulta en un rendimiento deficiente en el conjunto de validación. Es importante encontrar un equilibrio entre la complejidad del modelo (profundidad) y su capacidad para generalizar.

\subparagraph{Otros}
Además de los hiperparámetros mencionados anteriormente, existen otros que pueden ser ajustados, como la cantidad mínima de muestras por hoja (\textit{min\_split}), el número mínimo de observaciones requeridas en un nodo para realizar una división (\textit{min\_bucket}), y el criterio para medir la calidad de una división (\textit{split\_rule}).

\paragraph{Entrenamiento Random Forest y Validación Cruzada}\label{sec:entrenamiento-rf-validacion-cruzada}

En el proceso de construcción de un modelo de aprendizaje \textit{Random Forest}, es esencial comprender cómo se entrena un modelo de aprendizaje automático y cómo se evalúa su rendimiento. Para evaluar su rendimiento una herramienta muy útil es la validación cruzada, esta es una técnica fundamental para estimar el rendimiento del modelo de manera robusta. En esta sección, se abordarán estos dos aspectos clave del desarrollo de modelos los cuales están muy relacionados entre ellos.

\subparagraph{Entrenamiento \textit{Random Forest}}

El entrenamiento de un modelo de aprendizaje automático implica ajustar los hipeparámetros del modelo utilizando un conjunto de datos de entrenamiento, est proceso se ayuda de la validación cruzada para hacer modelos más robustos. Los pasos típicos en el entrenamiento de un modelo incluyen:

\begin{enumerate}  
  \item \textbf{División de datos:} Los datos se dividen en conjuntos de entrenamiento (\textit{test data}) y prueba (\textit{train data}). El conjunto de entrenamiento se utiliza para ajustar el modelo, mientras que el conjunto de prueba se reserva para evaluar su rendimiento, es decir el error de predicción.
  
  \item \textbf{Entrenamiento del modelo:} El modelo se ajusta utilizando el conjunto de entrenamiento. Esto implica encontrar los parámetros óptimos que minimizan una métrica de error o pérdida. Para ajustar los hiperparámetros también se puede hacer uso de la validación cruzada dentro de los datos de entrenamiento.
  
  \item \textbf{Evaluación del modelo:} Una vez entrenado, el modelo se evalúa utilizando el conjunto de prueba (\textit{test data}) para estimar su capacidad de generalización a datos no vistos por el modelo durante el entrenamiento.
\end{enumerate}

\subparagraph{Validación Cruzada en \textit{Random Forest}}

La validación cruzada es una técnica crucial para evaluar y seleccionar modelos de manera objetiva. En lugar de depender de una única división de datos en conjuntos de entrenamiento y prueba, la validación cruzada divide los datos en múltiples subconjuntos (pliegues) y realiza evaluaciones repetidas. Los pasos comunes de la validación cruzada son los siguientes:

\begin{enumerate}
  \item \textbf{División de datos:} Los datos se dividen en $k$ pliegues (por ejemplo, $k=5$ para validación cruzada de 5 pliegues).
  
  \item \textbf{Entrenamiento y evaluación repetidos:} El modelo se entrena y evalúa $k$ veces. En cada iteración, se utiliza un pliegue diferente como conjunto de prueba, mientras que los otros pliegues se utilizan para el entrenamiento.
  
  \item \textbf{Promedio de resultados:} Los resultados de las $k$ evaluaciones se promedian para obtener una estimación del rendimiento del modelo.
\end{enumerate}

La validación cruzada proporciona una evaluación más robusta y objetiva del rendimiento del modelo, ya que esta utiliza múltiples subconjuntos de prueba y permite detectar y paliar el sobreajuste. Es una práctica común en la evaluación de modelos de aprendizaje automático.

En resumen, tanto el entrenamiento de modelos como la validación cruzada son procesos esenciales en el desarrollo de modelos de aprendizaje automático. El entrenamiento se refiere a la configuración y ajuste de un modelo, mientras que la validación cruzada es una técnica para evaluar su rendimiento de manera objetiva y robusta. Ambos procesos se relacionan, en el fondo siempre todo parte de reservar datos y ver si el modelo cumple respecto al resto. Estos pasos son cruciales para garantizar que los modelos sean efectivos y generales en la predicción de nuevos datos


\vspace{10pt}

En resumen, los \textit{Random Forest} son una potente técnica de aprendizaje automático que combina múltiples árboles de clasificación (CART) para realizar predicciones más precisas y robustas. Su proceso de creación introduce variabilidad y utiliza técnicas como el OOB error y la importancia de variables para mejorar su rendimiento y capacidad de generalización.

